{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Law Citations Extractor for EUR-LEX\n",
    "Extracts case citations for all cases in EUR-LEX. This is done on the level-of-detail of the individual paragraph cited. For example, if we are extracting citations for case 62011CJ0488, then the citation 62010CJ0618: N 31 38 - 43 49 57 58 will be decomposed into the individual citations: 62010CJ0618: N31, 62010CJ0618: N38, 62010CJ0618: N39, 62010CJ0618: N40, 62010CJ0618: N41, 62010CJ0618: N42, 62010CJ0618: N43, 62010CJ0618: N49, 62010CJ0618: N57 and 62010CJ0618: N58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define main functions used in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: Part 1\n",
    "Low-level functions for actually extracting metadata of each type for the given source case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urllib library used to query a website\n",
    "from urllib.request import urlopen\n",
    "# BeautifulSoup webscraping module for python\n",
    "from bs4 import BeautifulSoup\n",
    "# CSV parser\n",
    "import csv\n",
    "# Regular expressions\n",
    "import regex\n",
    "\n",
    "#s = \"123123STRINGabcabc\"\n",
    "\n",
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "def find_between_r( s, first, last ):\n",
    "    try:\n",
    "        start = s.rindex( first ) + len( first )\n",
    "        end = s.rindex( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "#print find_between( s, \"123\", \"abc\" ) -> 123STRING\n",
    "#print find_between_r( s, \"123\", \"abc\" ) -> STRINGabc\n",
    "        \n",
    "def processProcedure(piece_of_text):\n",
    "    #print(piece_of_text)\n",
    "    # initialise list of items to be extracted from Procedure section\n",
    "    items = []\n",
    "    lines = piece_of_text.split('\\n')\n",
    "    lines2 = [x for x in lines if x]\n",
    "    lines3 = []\n",
    "    for thing in lines2:\n",
    "        if (thing != '' and thing != ' '):\n",
    "            lines3.append(thing)\n",
    "            \n",
    "    #print(lines3)\n",
    "    \n",
    "    judge = ''\n",
    "    advocate = ''\n",
    "    \n",
    "    for item in lines3:\n",
    "        \n",
    "        line_split = item.split(': ')\n",
    "        #print(line_split[0])\n",
    "        if (((line_split[0].upper().count('JUDGE') > 0) or (line_split[0].upper().count('RAPPORTEUR') > 0)) and (judge == '')):\n",
    "            judge = line_split[1]\n",
    "        elif ((line_split[0].upper().count('ADVOCATE') > 0) and (advocate == '')):\n",
    "            advocate = line_split[1]\n",
    "            \n",
    "    items.append(judge)\n",
    "    items.append(advocate)\n",
    "    \n",
    "    #print(items)\n",
    "    return items    \n",
    "            \n",
    "def processTitle(piece_of_text):\n",
    "    #print(\"text: \" + piece_of_text)\n",
    "    # initialise list of items to be extracted from Title section\n",
    "    items = []\n",
    "    lines = piece_of_text.split('\\n')\n",
    "    lines2 = [x for x in lines if x]\n",
    "    lines3 = []\n",
    "    for thing in lines2:\n",
    "        if (thing != '' and thing != ' '):\n",
    "            lines3.append(thing)\n",
    "            \n",
    "    #print(lines3)\n",
    "    \n",
    "    line_split = lines3[0].split('.')\n",
    "    line_split2 = [x for x in line_split if x]\n",
    "    line_split3 = []\n",
    "    for thing in line_split2:\n",
    "        if (thing != '' and thing != ' '):\n",
    "            line_split3.append(thing)\n",
    "    \n",
    "    #print(line_split3)\n",
    "    \n",
    "    ruling_title = line_split3[0]\n",
    "    chamber = find_between_r(line_split3[0], '(', ')')\n",
    "    ruling_name = line_split3[1]\n",
    "    \n",
    "    items.append(ruling_title)\n",
    "    items.append(chamber)\n",
    "    items.append(ruling_name)\n",
    "    \n",
    "    if (len(line_split3) == 5):\n",
    "        for k in range(2, len(line_split3)-1):\n",
    "            items.append(line_split3[k])\n",
    "    else:\n",
    "        items.append('Check EUR-LEX webpage')\n",
    "        items.append('Check EUR-LEX webpage')\n",
    "        \n",
    "    case_label = line_split3[len(line_split3)-1]\n",
    "    items.append(case_label)\n",
    "    ecli = lines3[len(lines3)-1]\n",
    "    items.append(ecli)\n",
    "        \n",
    "    #print(items)\n",
    "    return items       \n",
    "        \n",
    "        \n",
    "def processMisc(piece_of_text):\n",
    "    # initialise list of items to be extracted from Miscellaneous section (Country)\n",
    "    items = []\n",
    "    lines = piece_of_text.split('\\n')\n",
    "    lines2 = [x for x in lines if x]\n",
    "    lines3 = []\n",
    "    for thing in lines2:\n",
    "        if (thing != '' and thing != ' '):\n",
    "            lines3.append(thing)\n",
    "            \n",
    "    country = ''\n",
    "    \n",
    "    for item in lines3:\n",
    "        line_split = item.split(': ')\n",
    "        if (line_split[0].upper().count('COUNTRY') > 0):\n",
    "            country = line_split[1]\n",
    "            \n",
    "    items.append(country)\n",
    "    \n",
    "    #print(items)\n",
    "    return items    \n",
    "        \n",
    "        \n",
    "def processDates(piece_of_text):\n",
    "    # initialise list of items to be extracted from Dates section (lodged and document dates)\n",
    "    items = []\n",
    "    lines = piece_of_text.split('\\n')\n",
    "    lines2 = [x for x in lines if x]\n",
    "    dates = []\n",
    "    for thing in lines2:\n",
    "        if (thing != '' and thing != ' '):\n",
    "            dates.append(thing)\n",
    "            \n",
    "    lodge_date = ''\n",
    "    doc_date = ''\n",
    "    \n",
    "    for item in dates:\n",
    "        date_split = item.split(': ')\n",
    "        if (date_split[0].upper().count('LODGED') > 0):\n",
    "            lodge_date = date_split[1]\n",
    "        else:\n",
    "            doc_date = date_split[1]\n",
    "            \n",
    "    items.append(lodge_date)\n",
    "    items.append(doc_date)\n",
    "    \n",
    "    #print(items)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: Part 2\n",
    "1) Low-level function for actually extracting the citations for a given source case, 2) function for extracting other subject matters related to a case, and 3) function to write data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urllib library used to query a website\n",
    "from urllib.request import urlopen\n",
    "# BeautifulSoup webscraping module for python\n",
    "from bs4 import BeautifulSoup\n",
    "# CSV parser\n",
    "import csv\n",
    "# Regular expressions\n",
    "import regex\n",
    "\n",
    "# Write data (citations, metadata or subjects) to file\n",
    "def writeToFile(rows, datatype):\n",
    "    with open('../data/orders/'+datatype+'/orders_missing_advocates.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        # Open file for writing\n",
    "        writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        # Write each data row to file\n",
    "        # Check if any element of list is also a list\n",
    "        if (any(isinstance(el, list) for el in rows)):\n",
    "            for row in rows:\n",
    "                writer.writerow(row)\n",
    "        else:\n",
    "            writer.writerow(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: Part 3\n",
    "Main calling functions for extracting the citations and metadata for a given source case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import re\n",
    "from googletrans import Translator\n",
    "    \n",
    "def findSectionType(result):\n",
    "    results2 = result.find_all(\"div\")\n",
    "    for result2 in results2:\n",
    "        if result2.get('class') is not None:\n",
    "            if ((result2['class'][0]).count('boxTitle') == 1):\n",
    "                return result2.text\n",
    "    return ''\n",
    "\n",
    "def removeFirstAndLastHTMLTag(text):\n",
    "    if (text[0] == '<' and text[len(text)-1] == '>'):\n",
    "        endBracketPos = text.find('>')\n",
    "        if (endBracketPos > 0 and endBracketPos < len(text) - 1):\n",
    "            newText = text[1:]\n",
    "            beginBracketPos = newText.find('<')\n",
    "            return text[endBracketPos+1: (beginBracketPos+1)-len(text)]\n",
    "        else:\n",
    "            return text\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "def getIndex(text, phrase):\n",
    "    matches = []\n",
    "    for match in re.finditer(phrase, text):\n",
    "        matches.append(match.start())\n",
    "        print (match.start(), match.end())\n",
    "\n",
    "    if (len(matches) > 1):\n",
    "        return matches[len(matches)-1]\n",
    "    elif (len(matches) == 1):\n",
    "        return matches[0]\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def findPhrase(fullText, text):\n",
    "    phrases = []\n",
    "    result = []\n",
    "    \n",
    "    translator = Translator()\n",
    "    # English\n",
    "    phrases.append(text)\n",
    "    # French\n",
    "    french = translator.translate(text, dest='fr')\n",
    "    phrases.append(french.text)\n",
    "    # German\n",
    "    german = translator.translate(text, dest='de')\n",
    "    phrases.append(german.text)\n",
    "    # Italian\n",
    "    italian = translator.translate(text, dest='it')\n",
    "    phrases.append(italian.text)\n",
    "    # Spanish\n",
    "    spanish = translator.translate(text, dest='es')\n",
    "    phrases.append(spanish.text)\n",
    "    # Portuguese\n",
    "    portuguese = translator.translate(text, dest='pt')\n",
    "    phrases.append(portuguese.text)\n",
    "    # Dutch\n",
    "    dutch = translator.translate(text, dest='nl')\n",
    "    phrases.append(dutch.text)\n",
    "    # Greek\n",
    "    greek = translator.translate(text, dest='el')\n",
    "    phrases.append(greek.text)\n",
    "    # Polish\n",
    "    polish = translator.translate(text, dest='pl')\n",
    "    phrases.append(polish.text)\n",
    "    # Hungarian\n",
    "    hungarian = translator.translate(text, dest='hu')\n",
    "    phrases.append(hungarian.text)\n",
    "    # Swedish\n",
    "    swedish = translator.translate(text, dest='sv')\n",
    "    phrases.append(swedish.text)\n",
    "    # Romanian\n",
    "    romanian = translator.translate(text, dest='ro')\n",
    "    phrases.append(romanian.text)\n",
    "    # Turkish\n",
    "    turkish = translator.translate(text, dest='tr')\n",
    "    phrases.append(turkish.text)\n",
    "    # Croatian\n",
    "    croatian = translator.translate(text, dest='hr')\n",
    "    phrases.append(croatian.text)\n",
    "    # Bulgarian\n",
    "    bulgarian = translator.translate(text, dest='bg')\n",
    "    phrases.append(bulgarian.text)\n",
    "    # Czech\n",
    "    czech = translator.translate(text, dest='cs')\n",
    "    phrases.append(czech.text)\n",
    "    # Danish\n",
    "    danish = translator.translate(text, dest='da')\n",
    "    phrases.append(danish.text)\n",
    "    # Estonian\n",
    "    estonian = translator.translate(text, dest='et')\n",
    "    phrases.append(estonian.text)\n",
    "    # Finnish\n",
    "    finnish = translator.translate(text, dest='fi')\n",
    "    phrases.append(finnish.text)\n",
    "    # Irish\n",
    "    irish = translator.translate(text, dest='ga')\n",
    "    phrases.append(irish.text)\n",
    "    # Latvian\n",
    "    latvian = translator.translate(text, dest='lv')\n",
    "    phrases.append(latvian.text)\n",
    "    # Lithuanian\n",
    "    lithuanian = translator.translate(text, dest='lt')\n",
    "    phrases.append(lithuanian.text)\n",
    "    # Maltese\n",
    "    maltese = translator.translate(text, dest='mt')\n",
    "    phrases.append(maltese.text)\n",
    "    # Slovak\n",
    "    slovak = translator.translate(text, dest='sk')\n",
    "    phrases.append(slovak.text)\n",
    "    # Slovenian\n",
    "    slovenian = translator.translate(text, dest='sl')\n",
    "    phrases.append(slovenian.text)\n",
    "    \n",
    "    #print(len(phrases))\n",
    "    #print(phrases)\n",
    "    \n",
    "    numPhrases = len(phrases)\n",
    "    index = 0\n",
    "    \n",
    "    #print(index)\n",
    "    #print(phrases[index].text)\n",
    "    \n",
    "    phraseIndex = getIndex(fullText, phrases[index])\n",
    "    \n",
    "    while ((phraseIndex == -1) and (index < numPhrases-1)):\n",
    "        index += 1\n",
    "        #print(index)\n",
    "        #print(phrases[index].text)\n",
    "        phraseIndex = getIndex(fullText, phrases[index])\n",
    "     \n",
    "    if (index == numPhrases):\n",
    "        result.append(\"\")\n",
    "        result.append(-1)\n",
    "    else:\n",
    "        result.append(phrases[index])\n",
    "        result.append(phraseIndex)\n",
    "    \n",
    "    return result\n",
    "        \n",
    "# Extract Metadata for case given the BeautifulSoup format of it's HTML page\n",
    "def extractMetadata(soup_judgement_page, celexNumber):\n",
    "    text = str(soup_judgement_page)\n",
    "    phrase = \"Advocate General:\"\n",
    "    phraseAndIndex = findPhrase(text, phrase)\n",
    "    advocateName = \"\"\n",
    "    # datarow to write to file\n",
    "    datarow = []   \n",
    "    \n",
    "    if (phraseAndIndex[1] != -1):\n",
    "        startLookingIndex = phraseAndIndex[1] + len(phraseAndIndex[0])\n",
    "        currentChar = text[startLookingIndex]\n",
    "        while ((currentChar != '<') and (startLookingIndex < len(text))):\n",
    "            advocateName += currentChar\n",
    "            startLookingIndex += 1\n",
    "            currentChar = text[startLookingIndex]\n",
    "        \n",
    "        advocateName = advocateName.replace(\":\", \"\")\n",
    "        advocateName = advocateName.replace(\";\", \"\")\n",
    "        advocateName = advocateName.replace(\",\", \"\")\n",
    "        advocateName = advocateName.rstrip()\n",
    "    else:\n",
    "        phrase = \"Advocate General :\"\n",
    "        phraseAndIndex = findPhrase(text, phrase)\n",
    "        advocateName = \"\"\n",
    "        # datarow to write to file\n",
    "        datarow = []   \n",
    "    \n",
    "        if (phraseAndIndex[1] != -1):\n",
    "            startLookingIndex = phraseAndIndex[1] + len(phraseAndIndex[0])\n",
    "            currentChar = text[startLookingIndex]\n",
    "            while ((currentChar != '<') and (startLookingIndex < len(text))):\n",
    "                advocateName += currentChar\n",
    "                startLookingIndex += 1\n",
    "                currentChar = text[startLookingIndex]\n",
    "        \n",
    "            advocateName = advocateName.replace(\":\", \"\")\n",
    "            advocateName = advocateName.replace(\";\", \"\")\n",
    "            advocateName = advocateName.replace(\",\", \"\")\n",
    "            advocateName = advocateName.rstrip()\n",
    "    \n",
    "    #Write metadata row for source case to file\n",
    "    datarow.append(advocateName)\n",
    "    writeToFile(datarow, 'metadata')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: Part 4\n",
    "Functions for processing a single case and processing all cases within a main case subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import json\n",
    "\n",
    "# Process individual case given CELEX number\n",
    "def processCase(celexNumber):\n",
    "    print(\"Source: \" + str(celexNumber))\n",
    "    #print('-------------------------')\n",
    "    # URL prefix for a judgement on EUR-LEX\n",
    "    result_url_prefix = \"https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:\"\n",
    "    # URL for this particular judgement\n",
    "    judgement_url = result_url_prefix + str(celexNumber)\n",
    "    # Open the page\n",
    "    judgement_page = urlopen(judgement_url)\n",
    "    # Convert it to BeautifulSoup format \n",
    "    soup_judgement_page = BeautifulSoup(judgement_page, \"lxml\")\n",
    "    # Get Metadata\n",
    "    extractMetadata(soup_judgement_page, celexNumber)\n",
    "    # Get citations\n",
    "    #extractCitations(subjectMatterCode, soup_judgement_page, celexNumber)\n",
    "    #print()\n",
    "\n",
    " # Process cases for a particular subject matter code\n",
    "def processCases():\n",
    "    # URL prefix and suffix for judgements search results (url = prefix_url + subject matter code + suffix_url)\n",
    "    prefix_url = \"https://eur-lex.europa.eu/search.html?searchEq=true&qid=1535705893791&DB_TYPE_OF_ACT=judgment&CASE_LAW_SUMMARY=false&DTS_DOM=EU_LAW&typeOfActStatus=JUDGMENT&type=advanced&lang=en&SUBDOM_INIT=EU_CASE_LAW&DTS_SUBDOM=EU_CASE_LAW\"\n",
    "    #suffix_url = \"&typeOfActStatus=JUDGMENT&type=advanced&lang=en&SUBDOM_INIT=EU_CASE_LAW&DTS_SUBDOM=EU_CASE_LAW\"\n",
    "    # Get the URL for all cases about this subject matter code\n",
    "    url = prefix_url\n",
    "    # + subjectMatterCode + suffix_url\n",
    "    # Open the URL\n",
    "    url_page = urlopen(url)\n",
    "    # Parse the HTML in the page, and store them in Beautiful Soup format using the 'lxml' parser\n",
    "    soup_url_page = BeautifulSoup(url_page, \"lxml\")\n",
    "    # The search results page displays 10 results at a time. \n",
    "    # Get the total number of 10-result pages for this subject matter.    \n",
    "    j_onsubmit = soup_url_page.find('form', id='pagingForm').get('onsubmit')\n",
    "    j_onsubmit = j_onsubmit.replace(\" \", \"\")\n",
    "    j_number = 1\n",
    "    if (j_onsubmit):\n",
    "        j_numberStr = j_onsubmit.split(\",\")[1] \n",
    "        j_numberStr = j_numberStr.replace(\")\", \"\");\n",
    "        j_numberStr = j_numberStr.replace(\" \", \"\");\n",
    "        # Final number of result pages\n",
    "        j_number = int(j_numberStr)\n",
    "    else:\n",
    "        j_scripts = soup_url_page.find_all('script', type='application/json')\n",
    "        for script in j_scripts:\n",
    "            print(\"script: \" + script.text)\n",
    "            script_text = json.loads(script.text)\n",
    "            if 'search' in script_text:\n",
    "                if (script_text['search']):\n",
    "                    tmp = script_text['search']\n",
    "                    num = tmp['count']\n",
    "                    print(\"yay!: \" + str(num))\n",
    "                    if ((num/10) > 1):\n",
    "                        j_number = ceil(num/10)\n",
    "                \n",
    "    \n",
    "    print()\n",
    "    print(\"total pages: \" + str(j_number))\n",
    "    print()\n",
    "    # Get each result item tag (thats where the metadata for each case is found - including the CELEX number)\n",
    "    j_results_on_first_page = soup_url_page.find_all('td', class_='leftMetadata')\n",
    "            \n",
    "    print()\n",
    "    print(\"Page 1/\" + str(j_number))\n",
    "    print()\n",
    "    # Process the 10 cases on the first page of the results\n",
    "    for result in j_results_on_first_page:\n",
    "        for ul in result.find_all('ul'):        \n",
    "            for li in ul.find_all('li'):\n",
    "                if (li.text[:13] == \"CELEX number:\"):\n",
    "                    celex = li.text[14:]\n",
    "                    if (celex[0] == '6' and celex[5] == 'C') and (celex[6] == 'J' or celex[6] == 'O'):\n",
    "                        processCase(subjectMatterCode, celex)\n",
    "    \n",
    "    if (j_number > 1):\n",
    "        # Process the other cases from Page 2 of results onwards\n",
    "        for x in range(2, j_number+1):\n",
    "            print()\n",
    "            print(\"Page \" + str(x) + \"/\" + str(j_number))\n",
    "            print()\n",
    "            # Get URL of Page x of results\n",
    "            current_judgements_result_page_url = url + '&page=' + str(x) \n",
    "            # Open URL of Page x of results\n",
    "            current_judgements_result_page = urlopen(current_judgements_result_page_url)\n",
    "            # Store the HTML form of this page in BeautifulSoup format\n",
    "            soup_current_judgements_page = BeautifulSoup(current_judgements_result_page, \"lxml\")\n",
    "            # Get each result item tag (thats where the metadata for each case is found - including the CELEX number)\n",
    "            results_on_page_x = soup_current_judgements_page.find_all('td', class_='leftMetadata')\n",
    "            #Find the 10 cases on this page\n",
    "            for result in results_on_page_x:\n",
    "                for ul in result.find_all('ul'):\n",
    "                    for li in ul.find_all('li'):\n",
    "                        if (li.text[:13] == \"CELEX number:\"):\n",
    "                            celex = li.text[14:]\n",
    "                            if (celex[0] == '6' and celex[5] == 'C') and (celex[6] == 'J' or celex[6] == 'O'):\n",
    "                                processCase(subjectMatterCode, celex)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Procedure: Get the list of EUR-LEX subject matter codes from file and process the cases for each\n",
    "Note: we have a list of all the EUR-LEX subject matters in \"../data/SubjectMatterCodes.tsv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/104\n",
      "Source: 61995CO0012\n",
      "2/104\n",
      "Source: 61989CO0385\n",
      "3/104\n",
      "Source: 61981CO0232\n",
      "4/104\n",
      "Source: 61979CO0809\n",
      "5/104\n",
      "Source: 61978CO0243\n",
      "6/104\n",
      "Source: 61978CO0166\n",
      "7/104\n",
      "Source: 61978CO0092\n",
      "8/104\n",
      "Source: 61976CO0088\n",
      "9/104\n",
      "Source: 61975CO0044\n",
      "10/104\n",
      "Source: 61974CO0071(01)\n",
      "11/104\n",
      "Source: 61974CO0071\n",
      "12/104\n",
      "Source: 61962CO0025(01)\n",
      "13/104\n",
      "Source: 61962CO0025\n",
      "14/104\n",
      "Source: 61979CO0031\n",
      "15/104\n",
      "Source: 61973CO0004(01)\n",
      "16/104\n",
      "Source: 61975CO0109\n",
      "17/104\n",
      "Source: 61975CO0003\n",
      "18/104\n",
      "Source: 61973CO0160(01)\n",
      "19/104\n",
      "Source: 61973CO0160\n",
      "20/104\n",
      "Source: 61966CO0001\n",
      "21/104\n",
      "Source: 61965CO0053\n",
      "22/104\n",
      "Source: 61965CO0002\n",
      "23/104\n",
      "Source: 61964CO0036\n",
      "24/104\n",
      "Source: 61963CO0060\n",
      "25/104\n",
      "Source: 61959CO0031\n",
      "26/104\n",
      "Source: 61959CO0002\n",
      "27/104\n",
      "Source: 62011CO0172\n",
      "28/104\n",
      "Source: 62011CO0507\n",
      "29/104\n",
      "Source: 62010CO0446\n",
      "30/104\n",
      "Source: 62010CO0373\n",
      "31/104\n",
      "Source: 62006CO0122(01)\n",
      "32/104\n",
      "Source: 62000CO0278\n",
      "33/104\n",
      "Source: 61982CO0043\n",
      "34/104\n",
      "Source: 61981CO0060\n",
      "35/104\n",
      "Source: 61978CO0209\n",
      "36/104\n",
      "Source: 61976CO0026\n",
      "37/104\n",
      "Source: 61976CO0027\n",
      "38/104\n",
      "Source: 61972CO0006(01)\n",
      "39/104\n",
      "Source: 61974CO0071(01)\n",
      "40/104\n",
      "Source: 61974CO0071\n",
      "41/104\n",
      "Source: 61974CO0020(01)\n",
      "42/104\n",
      "Source: 61974CO0020\n",
      "43/104\n",
      "Source: 61973CO0006\n",
      "44/104\n",
      "Source: 61972CO0006\n",
      "45/104\n",
      "Source: 61971CO0045\n",
      "46/104\n",
      "Source: 62006CO0122(01)\n",
      "47/104\n",
      "Source: 62010CO0605\n",
      "48/104\n",
      "Source: 62008CO0114(01)\n",
      "49/104\n",
      "Source: 62011CO0002\n",
      "50/104\n",
      "Source: 62011CO0003\n",
      "51/104\n",
      "Source: 62007CO0463\n",
      "52/104\n",
      "Source: 62007CO0462\n",
      "53/104\n",
      "Source: 61977CO0121\n",
      "54/104\n",
      "Source: 61977CO0119\n",
      "55/104\n",
      "Source: 61977CO0113\n",
      "56/104\n",
      "Source: 61962CO0025(01)\n",
      "57/104\n",
      "Source: 61962CO0025\n",
      "58/104\n",
      "Source: 61994CO0097\n",
      "59/104\n",
      "Source: 62010CO0530\n",
      "60/104\n",
      "Source: 62008CO0114(01)\n",
      "61/104\n",
      "Source: 61997CO0248\n",
      "62/104\n",
      "Source: 61962CO0025(01)\n",
      "63/104\n",
      "Source: 61962CO0025\n",
      "64/104\n",
      "Source: 62011CO0172\n",
      "65/104\n",
      "Source: 62011CO0172\n",
      "66/104\n",
      "Source: 62011CO0569\n",
      "67/104\n",
      "Source: 62011CO0570\n",
      "68/104\n",
      "Source: 62015CO0061(01)\n",
      "69/104\n",
      "Source: 62017CO0327\n",
      "35739 35756\n",
      "70/104\n",
      "Source: 62010CO0605\n",
      "71/104\n",
      "Source: 62014CO0450\n",
      "72/104\n",
      "Source: 62008CO0114(01)\n",
      "73/104\n",
      "Source: 62017CO0453\n",
      "36530 36547\n",
      "74/104\n",
      "Source: 61995CO0254\n",
      "75/104\n",
      "Source: 61980CO0161(01)\n",
      "76/104\n",
      "Source: 61980CO0123(01)\n",
      "77/104\n",
      "Source: 61979CO0731(01)\n",
      "78/104\n",
      "Source: 61979CO0731\n",
      "79/104\n",
      "Source: 61979CO0051\n",
      "80/104\n",
      "Source: 61979CO0048\n",
      "81/104\n",
      "Source: 61978CO0019\n",
      "82/104\n",
      "Source: 61978CO0004\n",
      "83/104\n",
      "Source: 61976CO0061(01)\n",
      "84/104\n",
      "Source: 61976CO0091\n",
      "85/104\n",
      "Source: 61976CO0061\n",
      "86/104\n",
      "Source: 61975CO0054\n",
      "87/104\n",
      "Source: 61975CO0022\n",
      "88/104\n",
      "Source: 61974CO0062\n",
      "89/104\n",
      "Source: 61974CO0023\n",
      "90/104\n",
      "Source: 61972CO0075\n",
      "91/104\n",
      "Source: 61968CO0017\n",
      "92/104\n",
      "Source: 61968CO0027\n",
      "93/104\n",
      "Source: 61966CO0029\n",
      "94/104\n",
      "Source: 61965CO0028\n",
      "95/104\n",
      "Source: 61965CO0018\n",
      "96/104\n",
      "Source: 61964CO0030\n",
      "97/104\n",
      "Source: 61964CO0017\n",
      "98/104\n",
      "Source: 61963CO0098\n",
      "99/104\n",
      "Source: 61963CO0068\n",
      "100/104\n",
      "Source: 61962CO0035\n",
      "101/104\n",
      "Source: 61963CO0065\n",
      "102/104\n",
      "Source: 61963CO0015(01)\n",
      "103/104\n",
      "Source: 62015CO0061(01)\n",
      "104/104\n",
      "Source: 61958CO0003\n"
     ]
    }
   ],
   "source": [
    "# CSV parser\n",
    "import csv\n",
    "\n",
    "#processCase(\"62001CJ0164\")\n",
    "\n",
    "# Arrays for celex numbers\n",
    "#judgementsCelexNumbers = []\n",
    "ordersCelexNumbers = []\n",
    "\n",
    "# Import celex numbers from judgements metadata CSV to array\n",
    "# with open('../data/judgements/blank_advocates.csv', encoding=\"utf8\") as tsvfile:\n",
    "#     reader = csv.reader(tsvfile, delimiter=',')\n",
    "#     for row in reader:\n",
    "#         judgementsCelexNumbers.append(row[0])\n",
    "\n",
    "# Import celex numbers from orders metadata CSV to array\n",
    "with open('../data/orders/blank_advocates.csv', encoding=\"utf8\") as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        ordersCelexNumbers.append(row[0])\n",
    "        \n",
    "#print(len(judgementsCelexNumbers))\n",
    "# print(len(ordersCelexNumbers))\n",
    "#print(judgementsCelexNumbers)\n",
    "# print(ordersCelexNumbers)\n",
    "\n",
    "#judgementsCelexNumbers.pop(0)\n",
    "#ordersCelexNumbers.pop(0)\n",
    "\n",
    "index = 0\n",
    "length = len(ordersCelexNumbers)\n",
    "# For each celex number in the array\n",
    "for celexNumber in ordersCelexNumbers:\n",
    "    if (index+1 >= 1):\n",
    "        print(str(index+1) + \"/\" + str(length))\n",
    "        processCase(celexNumber)\n",
    "    index = index + 1\n",
    "    \n",
    "\n",
    "#     print(subjectMatterCode)\n",
    "#     processCases(subjectMatterCode)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
