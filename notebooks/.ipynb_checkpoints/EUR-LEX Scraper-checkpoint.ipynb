{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Law Citations Extractor for EUR-LEX\n",
    "Extracts case citations for all cases in EUR-LEX. This is done on the level-of-detail of the individual paragraph cited. For example, if we are extracting citations for case 62011CJ0488, then the citation 62010CJ0618: N 31 38 - 43 49 57 58 will be decomposed into the individual citations: 62010CJ0618: N31, 62010CJ0618: N38, 62010CJ0618: N39, 62010CJ0618: N40, 62010CJ0618: N41, 62010CJ0618: N42, 62010CJ0618: N43, 62010CJ0618: N49, 62010CJ0618: N57 and 62010CJ0618: N58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define main functions used in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: Part 1\n",
    "Low-level functions for actually extracting metadata of each type for the given source case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urllib library used to query a website\n",
    "from urllib.request import urlopen\n",
    "# BeautifulSoup webscraping module for python\n",
    "from bs4 import BeautifulSoup\n",
    "# CSV parser\n",
    "import csv\n",
    "# Regular expressions\n",
    "import re\n",
    "\n",
    "#s = \"123123STRINGabcabc\"\n",
    "\n",
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "def find_between_r( s, first, last ):\n",
    "    try:\n",
    "        start = s.rindex( first ) + len( first )\n",
    "        end = s.rindex( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "#print find_between( s, \"123\", \"abc\" ) -> 123STRING\n",
    "#print find_between_r( s, \"123\", \"abc\" ) -> STRINGabc\n",
    "        \n",
    "def processProcedure(piece_of_text):\n",
    "    #print(piece_of_text)\n",
    "    # initialise list of items to be extracted from Procedure section\n",
    "    items = []\n",
    "    lines = piece_of_text.split('\\n')\n",
    "    lines2 = [x for x in lines if x]\n",
    "    lines3 = []\n",
    "    for thing in lines2:\n",
    "        if (thing != '' and thing != ' '):\n",
    "            lines3.append(thing)\n",
    "            \n",
    "    #print(lines3)\n",
    "    \n",
    "    judge = ''\n",
    "    advocate = ''\n",
    "    \n",
    "    for item in lines3:\n",
    "        \n",
    "        line_split = item.split(': ')\n",
    "        #print(line_split[0])\n",
    "        if (((line_split[0].upper().count('JUDGE') > 0) or (line_split[0].upper().count('RAPPORTEUR') > 0)) and (judge == '')):\n",
    "            judge = line_split[1]\n",
    "        elif ((line_split[0].upper().count('ADVOCATE') > 0) and (advocate == '')):\n",
    "            advocate = line_split[1]\n",
    "            \n",
    "    items.append(judge)\n",
    "    items.append(advocate)\n",
    "    \n",
    "    #print(items)\n",
    "    return items    \n",
    "            \n",
    "def processTitle(piece_of_text):\n",
    "    # initialise list of items to be extracted from Title section\n",
    "    items = []\n",
    "    lines = piece_of_text.split('\\n')\n",
    "    lines2 = [x for x in lines if x]\n",
    "    lines3 = []\n",
    "    for thing in lines2:\n",
    "        if (thing != '' and thing != ' '):\n",
    "            lines3.append(thing)\n",
    "            \n",
    "    #print(lines3)\n",
    "    \n",
    "    line_split = lines3[0].split('.')\n",
    "    line_split2 = [x for x in line_split if x]\n",
    "    line_split3 = []\n",
    "    for thing in line_split2:\n",
    "        if (thing != '' and thing != ' '):\n",
    "            line_split3.append(thing)\n",
    "    \n",
    "    #print(line_split3)\n",
    "    \n",
    "    ruling_title = line_split3[0]\n",
    "    chamber = find_between_r(line_split3[0], '(', ')')\n",
    "    ruling_name = line_split3[1]\n",
    "    \n",
    "    items.append(ruling_title)\n",
    "    items.append(chamber)\n",
    "    items.append(ruling_name)\n",
    "    \n",
    "    if (len(line_split3) == 5):\n",
    "        for k in range(2, len(line_split3)-1):\n",
    "            items.append(line_split3[k])\n",
    "    else:\n",
    "        items.append('Check EUR-LEX webpage')\n",
    "        items.append('Check EUR-LEX webpage')\n",
    "        \n",
    "    case_label = line_split3[len(line_split3)-1]\n",
    "    items.append(case_label)\n",
    "    ecli = lines3[len(lines3)-1]\n",
    "    items.append(ecli)\n",
    "        \n",
    "    #print(items)\n",
    "    return items       \n",
    "        \n",
    "        \n",
    "def processMisc(piece_of_text):\n",
    "    # initialise list of items to be extracted from Miscellaneous section (Country)\n",
    "    items = []\n",
    "    lines = piece_of_text.split('\\n')\n",
    "    lines2 = [x for x in lines if x]\n",
    "    lines3 = []\n",
    "    for thing in lines2:\n",
    "        if (thing != '' and thing != ' '):\n",
    "            lines3.append(thing)\n",
    "            \n",
    "    country = ''\n",
    "    \n",
    "    for item in lines3:\n",
    "        line_split = item.split(': ')\n",
    "        if (line_split[0].upper().count('COUNTRY') > 0):\n",
    "            country = line_split[1]\n",
    "            \n",
    "    items.append(country)\n",
    "    \n",
    "    #print(items)\n",
    "    return items    \n",
    "        \n",
    "        \n",
    "def processDates(piece_of_text):\n",
    "    # initialise list of items to be extracted from Dates section (lodged and document dates)\n",
    "    items = []\n",
    "    lines = piece_of_text.split('\\n')\n",
    "    lines2 = [x for x in lines if x]\n",
    "    dates = []\n",
    "    for thing in lines2:\n",
    "        if (thing != '' and thing != ' '):\n",
    "            dates.append(thing)\n",
    "            \n",
    "    lodge_date = ''\n",
    "    doc_date = ''\n",
    "    \n",
    "    for item in dates:\n",
    "        date_split = item.split(': ')\n",
    "        if (date_split[0].upper().count('LODGED') > 0):\n",
    "            lodge_date = date_split[1]\n",
    "        else:\n",
    "            doc_date = date_split[1]\n",
    "            \n",
    "    items.append(lodge_date)\n",
    "    items.append(doc_date)\n",
    "    \n",
    "    #print(items)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: Part 2\n",
    "1) Low-level function for actually extracting the citations for a given source case, 2) function for extracting other subject matters related to a case, and 3) function to write data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urllib library used to query a website\n",
    "from urllib.request import urlopen\n",
    "# BeautifulSoup webscraping module for python\n",
    "from bs4 import BeautifulSoup\n",
    "# CSV parser\n",
    "import csv\n",
    "# Regular expressions\n",
    "import re\n",
    "\n",
    "# Write data (citations, metadata or subjects) to file\n",
    "def writeToFile(subjectMatterCode, rows, datatype):\n",
    "    with open('../data/judgements/'+datatype+'/'+subjectMatterCode+'_judgements_'+datatype+'.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        # Open file for writing\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        # Write each data row to file\n",
    "        # Check if any element of list is also a list\n",
    "        if (any(isinstance(el, list) for el in rows)):\n",
    "            for row in rows:\n",
    "                writer.writerow(row)\n",
    "        else:\n",
    "            writer.writerow(rows)\n",
    "\n",
    "# Extract other subject matters for this source case (process the Classification section of the page)\n",
    "def processClassifications(subjectMatterCode, piece_of_text, celexNumber):\n",
    "    subjectMatters = find_between_r(piece_of_text, \"Subject matter:\", \"Case law directory code:\")\n",
    "    splitStr = subjectMatters.split('\\n')\n",
    "    splitStr2 = [x for x in splitStr if x]\n",
    "    subjectMattersFinal = []\n",
    "    for thing in splitStr2:\n",
    "        if (thing != '' and thing != ' '):\n",
    "            subjectMattersFinal.append(thing)\n",
    "    \n",
    "    datarows = []\n",
    "    for item in subjectMattersFinal:\n",
    "        datarow = []\n",
    "        datarow.append(celexNumber)\n",
    "        tmp = item.replace(',', ';')\n",
    "        tmp2 = tmp.replace('|', '')\n",
    "        datarow.append(tmp2)\n",
    "        datarows.append(datarow)\n",
    "            \n",
    "    writeToFile(subjectMatterCode, datarows, 'subjects')\n",
    "        \n",
    "# Filter out case citations that we are not interested in\n",
    "def decomposeAndValidateCitation(citation, list_item):\n",
    "    #print('citation: ' + str(citation))\n",
    "    # Correct citations array\n",
    "    correct_citations = []\n",
    "    citation = citation.replace(\" \", \"\")\n",
    "    # Check if citation is a case (not a legislation or directive etc.)\n",
    "    if (citation) and (citation[0] == '6' and citation[5] == 'C') and (citation[6] == 'J' or citation[6] == 'O'):\n",
    "        # It is a valid citation, now decompose it\n",
    "        # Part A: clean the last part of the citation e.g. the '-N10-14' part in '61980CJ0100-N10-14' -> '61980CJ0100: N10,\n",
    "        # 61980CJ0100: N11, 61980CJ0100: N12, 61980CJ0100: N13, 61980CJ0100: N14'\n",
    "        #print('----------------------------------------------------------')        \n",
    "        #print() \n",
    "        #print('Citation: ' + str(citation))\n",
    "        part_to_clean = citation[11:]\n",
    "        if (part_to_clean.count(',') > 0):\n",
    "            print('WTF')\n",
    "            part_to_clean = part_to_clean.replace(',', ' ')\n",
    "        #print('Part to clean: ' + str(part_to_clean))\n",
    "        dash_count = part_to_clean.count('-')\n",
    "        n_count = part_to_clean.count('N')\n",
    "        if ((dash_count > 1) or (n_count > 1)):\n",
    "            if (dash_count > 1):\n",
    "                new_part_to_clean = part_to_clean[1:]\n",
    "                #print('To Clean: ' + str(new_part_to_clean))\n",
    "                guess = \"-\"\n",
    "                occurrences = new_part_to_clean.count(guess)\n",
    "                indices = [i for i, a in enumerate(new_part_to_clean) if a == guess]\n",
    "                #print('Dashes: ' + str(indices))\n",
    "                \n",
    "                split_part = new_part_to_clean.split('-')\n",
    "                lhs = split_part[0]\n",
    "                rhs = split_part[1]\n",
    "                number1 = re.sub(\"[^0-9]\", \"\", lhs)\n",
    "                number2 = re.sub(\"[^0-9]\", \"\", rhs)\n",
    "                for x in range(int(number1), int(number2)+1):\n",
    "                    current_correct_citation = str(citation[:11]) + ': N' + str(x)\n",
    "                    correct_citations.append(current_correct_citation)\n",
    "            else:\n",
    "                # It must be the case that there are two N's and a :\n",
    "                # Remove trailing - and split on :\n",
    "                new_part_to_clean = part_to_clean[1:].split(':')\n",
    "                current_correct_citation = str(citation[:11]) + ': ' + str(new_part_to_clean[0])\n",
    "                correct_citations.append(current_correct_citation)\n",
    "                rhs = new_part_to_clean[1]\n",
    "                rhs_numbers = rhs.split(' ')\n",
    "                for rhs_number in rhs_numbers:\n",
    "                    tmp = re.sub(\"[^0-9]\", \"\", rhs_number)\n",
    "                    if (len(tmp) != 0):\n",
    "                        current_correct_citation = str(citation[:11]) + ': N' + str(rhs_number)\n",
    "                        correct_citations.append(current_correct_citation)\n",
    "        else:\n",
    "            split_part = citation.split('-')\n",
    "            #print('len split: ' + str(len(split_part)))\n",
    "            if (len(split_part) > 1):\n",
    "                correct_citation = str(split_part[0]) + ': ' + str(split_part[1]) \n",
    "                #print('Correct citation: ' + str(correct_citation))\n",
    "                correct_citations.append(correct_citation)\n",
    "            \n",
    "        # Part B: add other paragraphs \n",
    "        # E.g. '61980CJ0100-N10-14: N 142' should include Part A decompositions + '61980CJ0100: N142'\n",
    "        #print('another: ' + str(citation))\n",
    "        test = list_item.text\n",
    "        string_result = test.split('\\n')\n",
    "        for tmp in string_result:\n",
    "            if (tmp.find(str(citation) + ':') != -1):\n",
    "                #print('Part B: ' + str(tmp))\n",
    "                split_again = tmp.split(':')\n",
    "                stuff_to_clean = split_again[1]\n",
    "                \n",
    "                stuff_to_clean = stuff_to_clean.replace('-', ' - ')\n",
    "                stuff_to_clean = re.sub(' +',' ',stuff_to_clean)\n",
    "                #print('Stuff to clean: ' + str(stuff_to_clean))\n",
    "                if (stuff_to_clean.count(',') > 0):\n",
    "                    print('WTF2')\n",
    "                    stuff_to_clean = stuff_to_clean.replace(',', ' ')\n",
    "                #print('-------------------------')\n",
    "                dash_count2 = 0\n",
    "                dash_count2 = stuff_to_clean.count('-')\n",
    "                if (dash_count2 > 0):\n",
    "#                     print('To Clean: ' + str(stuff_to_clean))\n",
    "#                     guess = \"-\"\n",
    "#                     occurrences = stuff_to_clean.count(guess)\n",
    "#                     indices = [i for i, a in enumerate(stuff_to_clean) if a == guess]\n",
    "#                     print('Dashes: ' + str(indices))\n",
    "                    \n",
    "                     \n",
    "                            \n",
    "#                     print('After cleaning: ' + str(stuff_to_clean))        \n",
    "                    \n",
    "#                     split_string_dash = stuff_to_clean.split('-')\n",
    "#                     lhs = split_string_dash[0]\n",
    "#                     split_lhs = lhs.split(' ')\n",
    "#                     split_lhs = list(filter(None, split_lhs))\n",
    "#                     rhs = split_string_dash[1]\n",
    "#                     split_rhs = rhs.split(' ')\n",
    "#                     split_rhs = list(filter(None, split_rhs))\n",
    "#                     last_num_lhs = split_lhs[len(split_lhs)-1]\n",
    "#                     first_num_rhs = split_rhs[0]\n",
    "            \n",
    "#                     for x in range(int(last_num_lhs), int(first_num_rhs)+1):\n",
    "#                         current_correct_citation = str(citation[:11]) + ': N' + str(x)\n",
    "#                         correct_citations.append(current_correct_citation)\n",
    "            \n",
    "                    split_string_space = stuff_to_clean.split(' ')\n",
    "                    for idx, val in enumerate(split_string_space):\n",
    "                        if (val == '-'):\n",
    "                            num1 = int(split_string_space[idx-1])\n",
    "                            num2 = int(split_string_space[idx+1])\n",
    "                            for x in range(num1, num2+1):\n",
    "                                current_correct_citation = str(citation[:11]) + ': N' + str(x)\n",
    "                                #print (\"Current correct citation: \" + str(current_correct_citation))\n",
    "                                correct_citations.append(current_correct_citation)\n",
    "                        else:\n",
    "                            tmp = re.sub(\"[^0-9]\", \"\", val)\n",
    "                            if (len(tmp) != 0):\n",
    "                                current_correct_citation = str(citation[:11]) + ': N' + str(val)\n",
    "                                #print (\"Current correct citation: \" + str(current_correct_citation))\n",
    "                                correct_citations.append(current_correct_citation)\n",
    "\n",
    "                else:\n",
    "                    split_string = stuff_to_clean.split(' ')\n",
    "                    for num in split_string:\n",
    "                        tmp = re.sub(\"[^0-9]\", \"\", num)\n",
    "                        if (len(tmp) != 0):\n",
    "                            #print(num)\n",
    "                            current_correct_citation = str(citation[:11]) + ': N' + str(num)\n",
    "                            #print (\"Correct citation: \" + str(current_correct_citation))\n",
    "                            correct_citations.append(current_correct_citation)\n",
    "                    #print('-------------------------')\n",
    "   \n",
    "    return correct_citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: Part 3\n",
    "Main calling functions for extracting the citations and metadata for a given source case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract citations for the case given the BeautifulSoup format of it's HTML page\n",
    "def extractCitations(subjectMatterCode, soup_judgement_page, celexNumber):\n",
    "    # Citations array\n",
    "    citations = []\n",
    "    # Get all list items in this web page (the citations are in one of the list items in the HTML source)\n",
    "    li_results = soup_judgement_page.find_all('li')\n",
    "    # loop through items until you find the citation list item\n",
    "    for result in li_results:\n",
    "        # Check if this is the list item that lists all the citations for this judgement\n",
    "        if (result.text[:11] == 'Instruments'):\n",
    "            # If it is, loop through each, extract the citations, and write them to csv\n",
    "            for link in result.find_all('a'):\n",
    "                #if there is an href attribute\n",
    "                if (link.has_attr('href')):\n",
    "                    #extract the case number from the href\n",
    "                    this_citation = link.text\n",
    "                    #print(\"THIS CITATION:\" + str(this_citation))\n",
    "                    decomposedCitations = decomposeAndValidateCitation(this_citation, result)\n",
    "                    # Remove empty items\n",
    "                    decomposedCitations2 = list(filter(None, decomposedCitations))\n",
    "                    for citation in decomposedCitations2:\n",
    "                        if citation not in citations:\n",
    "                            #print(\"Final correct citation: \" + str(citation))\n",
    "                            citations.append(citation)\n",
    "                            \n",
    "    # Initialise list of new data rows             \n",
    "    datarows = []\n",
    "    \n",
    "    # For each citation\n",
    "    for item in citations:\n",
    "        # Initialise a new data row\n",
    "        datarow = []\n",
    "        # Split current citation into case code and paragraph number (e.g. 62008CJ0040: N 44 -> '62008CJ0040' + 'N44')\n",
    "        split_citation = item.split(': ')\n",
    "        # Add source case code to new data row\n",
    "        datarow.append(celexNumber)\n",
    "        # Add citation case code to new data row\n",
    "        datarow.append(split_citation[0])\n",
    "        # Add paragraph number to new data row\n",
    "        datarow.append(split_citation[1])\n",
    "        # Add new data row to the total data rows\n",
    "        datarows.append(datarow)\n",
    "     \n",
    "    # Write total data rows to citations file\n",
    "    writeToFile(subjectMatterCode, datarows, 'citations')\n",
    "    \n",
    "def findSectionType(result):\n",
    "    results2 = result.find_all(\"div\")\n",
    "    for result2 in results2:\n",
    "        if result2.get('class') is not None:\n",
    "            if ((result2['class'][0]).count('boxTitle') == 1):\n",
    "                return result2.text\n",
    "    return ''\n",
    "\n",
    "# Extract Metadata for case given the BeautifulSoup format of it's HTML page\n",
    "def extractMetadata(subjectMatterCode, soup_judgement_page, celexNumber):\n",
    "    # find all divs of class 'box'\n",
    "    div_results = soup_judgement_page.find_all(\"div\", {\"class\": \"box\"})\n",
    "    # sections\n",
    "    sections = []\n",
    "    # datarow to write to file\n",
    "    datarow = []\n",
    "    datarow.append(celexNumber)\n",
    "    # for each div of class 'box'\n",
    "    for result in div_results:\n",
    "        results2 = result.find_all(\"div\")\n",
    "        index = 0\n",
    "        sectionType = findSectionType(result)\n",
    "        sectionType = sectionType.replace(\" \", \"\")\n",
    "        #print(\"sectionType: \" + str(sectionType))\n",
    "        if (sectionType.count(\"Titleandreference\") > 0) or (sectionType.count(\"Dates\") > 0) or (sectionType.count(\"Procedure\") > 0) or (sectionType.count(\"Classifications\") > 0) or (sectionType.count(\"Miscellaneousinformation\") > 0):\n",
    "            #print(\"sectionType2: \" + sectionType)\n",
    "            # for each div inside \n",
    "            for result2 in results2:\n",
    "                if result2.get('class') is not None:\n",
    "                    if ((result2['class'][0]).count('tabContent') == 1):\n",
    "                        if (sectionType.count(\"Titleandreference\") > 0):\n",
    "                            # 1. Title and reference\n",
    "                            # Chamber, ruling name, ruling content, case label\n",
    "                            #print(result2.text)\n",
    "                            title = processTitle(result2.text)\n",
    "                            datarow.extend(title)\n",
    "                            \n",
    "                        elif (sectionType.count(\"Dates\") > 0):\n",
    "                            # 2. Dates\n",
    "                            # Date document, date lodged\n",
    "                            #print(result2.text)\n",
    "                            dates = processDates(result2.text)\n",
    "                            datarow.extend(dates)\n",
    "                            \n",
    "                        elif (sectionType.count(\"Classifications\") > 0):\n",
    "                            #print(result2.text)\n",
    "                            # 3. Classifications\n",
    "                            # 3a. Subject matters\n",
    "                            processClassifications(subjectMatterCode, result2.text, celexNumber)\n",
    "                            \n",
    "                        elif (sectionType.count(\"Miscellaneousinformation\") > 0):\n",
    "                            # 4. Misc\n",
    "                            # Country\n",
    "                            misc = processMisc(result2.text)\n",
    "                            datarow.extend(misc)\n",
    "                            \n",
    "                        elif (sectionType.count(\"Procedure\") > 0):\n",
    "                            # 5. Procedure\n",
    "                            # Judge-Rapporteur, Advocate General\n",
    "                            procedure = processProcedure(result2.text)\n",
    "                            datarow.extend(procedure)\n",
    "    \n",
    "    # Clean datarow items of all commas within each item\n",
    "    cleaned_row = []\n",
    "    for item in datarow:\n",
    "        tmp = item.replace(',', ';')\n",
    "        cleaned_row.append(tmp)\n",
    "        \n",
    "    cleaned_row.append(subjectMatterCode)\n",
    "    #print(cleaned_row)\n",
    "    #Write metadata row for source case to file\n",
    "    writeToFile(subjectMatterCode, cleaned_row, 'metadata')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions: Part 4\n",
    "Functions for processing a single case and processing all cases within a main case subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import json\n",
    "\n",
    "# Process individual case given CELEX number\n",
    "def processCase(subjectMatterCode, celexNumber):\n",
    "    print(\"Source: \" + str(celexNumber))\n",
    "    #print('-------------------------')\n",
    "    # URL prefix for a judgement on EUR-LEX\n",
    "    result_url_prefix = \"https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:\"\n",
    "    # URL for this particular judgement\n",
    "    judgement_url = result_url_prefix + str(celexNumber)\n",
    "    # Open the page\n",
    "    judgement_page = urlopen(judgement_url)\n",
    "    # Convert it to BeautifulSoup format \n",
    "    soup_judgement_page = BeautifulSoup(judgement_page, \"lxml\")\n",
    "    # Get Metadata\n",
    "    extractMetadata(subjectMatterCode, soup_judgement_page, celexNumber)\n",
    "    # Get citations\n",
    "    extractCitations(subjectMatterCode, soup_judgement_page, celexNumber)\n",
    "    #print()\n",
    "\n",
    " # Process cases for a particular subject matter code\n",
    "def processCases(subjectMatterCode):\n",
    "    # URL prefix and suffix for judgements search results (url = prefix_url + subject matter code + suffix_url)\n",
    "    prefix_url = \"http://eur-lex.europa.eu/search.html?searchEq=true&qid=1524797649507&DB_TYPE_OF_ACT=judgment&CASE_LAW_SUMMARY=false&DTS_DOM=EU_LAW&CT_CODED=\"\n",
    "    suffix_url = \"&typeOfActStatus=JUDGMENT&type=advanced&lang=en&SUBDOM_INIT=EU_CASE_LAW&DTS_SUBDOM=EU_CASE_LAW\"\n",
    "    # Get the URL for all cases about this subject matter code\n",
    "    url = prefix_url + subjectMatterCode + suffix_url\n",
    "    # Open the URL\n",
    "    url_page = urlopen(url)\n",
    "    # Parse the HTML in the page, and store them in Beautiful Soup format using the 'lxml' parser\n",
    "    soup_url_page = BeautifulSoup(url_page, \"lxml\")\n",
    "    # The search results page displays 10 results at a time. \n",
    "    # Get the total number of 10-result pages for this subject matter.    \n",
    "    j_onsubmit = soup_url_page.find('form', id='pagingForm').get('onsubmit')\n",
    "    j_onsubmit = j_onsubmit.replace(\" \", \"\")\n",
    "    j_number = 1\n",
    "    if (j_onsubmit):\n",
    "        j_numberStr = j_onsubmit.split(\",\")[1] \n",
    "        j_numberStr = j_numberStr.replace(\")\", \"\");\n",
    "        j_numberStr = j_numberStr.replace(\" \", \"\");\n",
    "        # Final number of result pages\n",
    "        j_number = int(j_numberStr)\n",
    "    else:\n",
    "        j_scripts = soup_url_page.find_all('script', type='application/json')\n",
    "        for script in j_scripts:\n",
    "            print(\"script: \" + script.text)\n",
    "            script_text = json.loads(script.text)\n",
    "            if 'search' in script_text:\n",
    "                if (script_text['search']):\n",
    "                    tmp = script_text['search']\n",
    "                    num = tmp['count']\n",
    "                    print(\"yay!: \" + str(num))\n",
    "                    if ((num/10) > 1):\n",
    "                        j_number = ceil(num/10)\n",
    "                \n",
    "    \n",
    "    print()\n",
    "    print(\"total pages: \" + str(j_number))\n",
    "    print()\n",
    "    # Get each result item tag (thats where the metadata for each case is found - including the CELEX number)\n",
    "    j_results_on_first_page = soup_url_page.find_all('td', class_='leftMetadata')\n",
    "            \n",
    "    print()\n",
    "    print(\"Page 1/\" + str(j_number))\n",
    "    print()\n",
    "    # Process the 10 cases on the first page of the results\n",
    "    for result in j_results_on_first_page:\n",
    "        for ul in result.find_all('ul'):        \n",
    "            for li in ul.find_all('li'):\n",
    "                if (li.text[:13] == \"CELEX number:\"):\n",
    "                    celex = li.text[14:]\n",
    "                    if (celex[0] == '6' and celex[5] == 'C') and (celex[6] == 'J' or celex[6] == 'O'):\n",
    "                        processCase(subjectMatterCode, celex)\n",
    "    \n",
    "    if (j_number > 1):\n",
    "        # Process the other cases from Page 2 of results onwards\n",
    "        for x in range(2, j_number+1):\n",
    "            print()\n",
    "            print(\"Page \" + str(x) + \"/\" + str(j_number))\n",
    "            print()\n",
    "            # Get URL of Page x of results\n",
    "            current_judgements_result_page_url = url + '&page=' + str(x) \n",
    "            # Open URL of Page x of results\n",
    "            current_judgements_result_page = urlopen(current_judgements_result_page_url)\n",
    "            # Store the HTML form of this page in BeautifulSoup format\n",
    "            soup_current_judgements_page = BeautifulSoup(current_judgements_result_page, \"lxml\")\n",
    "            # Get each result item tag (thats where the metadata for each case is found - including the CELEX number)\n",
    "            results_on_page_x = soup_current_judgements_page.find_all('td', class_='leftMetadata')\n",
    "            #Find the 10 cases on this page\n",
    "            for result in results_on_page_x:\n",
    "                for ul in result.find_all('ul'):\n",
    "                    for li in ul.find_all('li'):\n",
    "                        if (li.text[:13] == \"CELEX number:\"):\n",
    "                            celex = li.text[14:]\n",
    "                            if (celex[0] == '6' and celex[5] == 'C') and (celex[6] == 'J' or celex[6] == 'O'):\n",
    "                                processCase(subjectMatterCode, celex)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Procedure: Get the list of EUR-LEX subject matter codes from file and process the cases for each\n",
    "Note: we have a list of all the EUR-LEX subject matters in \"../data/SubjectMatterCodes.tsv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: 62000CJ0011\n",
      "sectionType: Titleandreference\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sectionType2: Titleandreference\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sectionType: Languagesandformatsavailable\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sectionType: Multilingualdisplay\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sectionType: Dates\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sectionType2: Dates\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sectionType: Procedure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sectionType2: Procedure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Applicant: COMM, IC\n",
      "\n",
      "Defendant: BCE, IC\n",
      "\n",
      "Judge-Rapporteur: La Pergola\n",
      "Advocate General: Jacobs\n",
      "\n",
      "\n",
      "sectionType: Doctrine\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sectionType: Relationshipbetweendocuments\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sectionType: Text\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sectionType: \n"
     ]
    }
   ],
   "source": [
    "# CSV parser\n",
    "import csv\n",
    "\n",
    "# Array storing EUR-LEX subject matter codes\n",
    "subjectMatterCodes = []\n",
    "\n",
    "# Transfer codes from TSV file to array\n",
    "with open('../data/subject_matters_sorted.csv') as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        subjectMatterCodes.append(row[0])\n",
    "\n",
    "\n",
    "#processCase('FIN', '62000CJ0011')\n",
    "# For each subject matter code in EUR-LEX\n",
    "for subjectMatterCode in subjectMatterCodes:\n",
    "    print(subjectMatterCode)\n",
    "    processCases(subjectMatterCode)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
